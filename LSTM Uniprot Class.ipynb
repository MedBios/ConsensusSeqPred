{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn \n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import Request, urlopen\n",
    "import numpy as np\n",
    "from skimage.util import view_as_windows as vaw\n",
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))\n",
    "import os, sys\n",
    "import glob\n",
    "from progressbar import ProgressBar\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.layers.recurrent import lstm\n",
    "from tflearn.layers.core import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniprot_keyword = 'keyword1+keyword2+keyword3' # if you want to do multiple search terms\n",
    "uniprot_keyword = 'antibody+DNA-binding' # what to query uniprot for\n",
    "string_len = 10 # how many amino acids to take for each substring\n",
    "uniprot_limit = 1000 # how many proteins to get for each class from uniprot\n",
    "# how many amino acids to skip during cutting when moving to next cut.\n",
    "# if this number is 1, it just moves to the next one.\n",
    "stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_tensorboard_dep():\n",
    "  if 'ngrok-stable-linux-amd64.zip' not in os.listdir(os.getcwd()):\n",
    "    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "    !unzip ngrok-stable-linux-amd64.zip\n",
    "    os.system('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_tensorboard(d):\n",
    "  LOG_DIR = d  # tells Tensorboard where to look for log files\n",
    "  get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))\n",
    "  get_ipython().system_raw('./ngrok http 6006 &')\n",
    "  ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "  \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniprot_data(kw, numxs):\n",
    "    '''Goes to the uniprot website and searches for \n",
    "       data with the keyword given. Returns the data \n",
    "       found up to limit elements.'''\n",
    "\n",
    "    kws = [kw, 'NOT+' + kw]\n",
    "    Protein_data = {}\n",
    "    \n",
    "    for i in range(2):\n",
    "        kw = kws[i]\n",
    "        url1 = 'http://www.uniprot.org/uniprot/?query='\n",
    "        url2 = '&columns=sequence&format=tab&limit='+str(numxs)\n",
    "        query_complete = url1 + kw + url2\n",
    "        request = Request(query_complete)\n",
    "        response = urlopen(request)\n",
    "        data = response.read()\n",
    "        data = str(data, 'utf-8')\n",
    "        data = data.split('\\n')\n",
    "        data = data[1:-1]\n",
    "        Protein_data[str(i)] = list(map(lambda x:x.lower(),data))\n",
    "                     \n",
    "    x = Protein_data['0'] + Protein_data['1']\n",
    "    y = np.zeros([len(x), ])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_strings(seqs, labels, length, padlen=None):\n",
    "    if padlen is None:\n",
    "        padlen = int(0.95 * length)\n",
    "\n",
    "    x = np.zeros([0, length, 1])\n",
    "    y = np.zeros([0, ])\n",
    "    count = 0\n",
    "    xlen = None\n",
    "    bar = ProgressBar()\n",
    "\n",
    "    for seq in bar(seqs):\n",
    "        seq_nums = []\n",
    "        for letter in seq:\n",
    "            seq_nums.append(max(ord(letter)-97, 0))\n",
    "\n",
    "        if len(seq_nums) > length:\n",
    "            padded_seq = np.pad(np.asarray(seq_nums), (padlen, padlen),\n",
    "                                          'constant', constant_values=22.)\n",
    "            cut_seq = vaw(padded_seq, (length, ))\n",
    "            y = np.concatenate((y, np.ones([cut_seq.shape[0], ])*labels[count]))\n",
    "            x = np.concatenate((x, cut_seq[..., None]))\n",
    "            count += 1\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    print('Used {} proteins.'.format(count))\n",
    "    plt.hist(x[x != 22.].flatten(), bins=25)\n",
    "    plt.show()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# call the get_uniprot_data function to get data and labels\n",
    "X, Y = get_uniprot_data(uniprot_keyword, uniprot_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = cut_strings(X, Y, string_len, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of the data...should be number of examples X length of each sequence\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(np.unique(X)) # see all of the different numbers in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(Y, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tflearn.data_utils.to_categorical(Y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_dir = ('/home/mpcr/DeepProteins')\n",
    "name = str(string_len) + '_seq_len_'\n",
    "name += str(stride) + '_stride'\n",
    "install_tensorboard_dep()\n",
    "start_tensorboard('tb_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_in = tflearn.input_data([None, string_len, 1])\n",
    "layer1 = lstm(net_in, 10, return_seq=True)\n",
    "layer2 = lstm(layer1, 10)\n",
    "out = fully_connected(layer2, 2, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tflearn.regression(out, optimizer='adam', learning_rate=0.0005,\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tflearn.DNN(net, tensorboard_verbose=2, tensorboard_dir=tb_dir)\n",
    "model.fit(X, Y, validation_set=0.25, show_metric=True, n_epoch=9,\n",
    "          batch_size=200, snapshot_step=200, run_id=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
